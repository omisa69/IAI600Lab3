{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LassoCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Handling Categorical Data\n",
    "cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Handling Heavy-Tailed features\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, base=np.e):\n",
    "        self.base = base\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_in_ = X.columns.to_list()\n",
    "        else:\n",
    "            self.feature_names_in_ = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.emath.logn(self.base, X)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features == None:\n",
    "            input_features = self.feature_names_in_\n",
    "        return [f\"log_{name}\" for name in input_features]\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return self.base ** X\n",
    "    \n",
    "log_transformer = LogTransformer()\n",
    "\n",
    "# Handling Geographic features\n",
    "\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters,random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
    "    \n",
    "cluster_simil_transformer = ClusterSimilarity(random_state=42)\n",
    "\n",
    "# Transforming housing_median_age\n",
    "quantile_transformer = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "\n",
    "# Creating Additional Features (bedrooms (bedrooms per room), rooms_per_house, people_per_house)\n",
    "class RatioFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_in_ = X.columns.to_list()\n",
    "        else:\n",
    "            self.feature_names_in_ = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if X.shape[1] < 2:             \n",
    "            raise ValueError(\"Ratio transformation requires at least two columns.\")\n",
    "        transformed_X = X[:, 0] / X[:, 1]\n",
    "        return transformed_X.reshape(-1,1)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features == None:\n",
    "            input_features = self.feature_names_in_\n",
    "        else:\n",
    "            return [f\"additional_{name}_ratio\" for name in input_features]\n",
    "\n",
    "\n",
    "ratio_features_transformer = RatioFeaturesTransformer()\n",
    "\n",
    "# Scaling\n",
    "standard_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deafult_number_pipeline = make_pipeline(num_imputer, standard_scaler)\n",
    "\n",
    "# Heavy-Tailed Features\n",
    "log_pipeline = make_pipeline(num_imputer, log_transformer, standard_scaler)\n",
    "\n",
    "# Categorical Features\n",
    "cat_pipeline = make_pipeline(cat_imputer, cat_encoder)\n",
    "\n",
    "# Geographic Features\n",
    "geo_pipeline = Pipeline([(\"cluster_simil_transformer\", cluster_simil_transformer)])\n",
    "\n",
    "# median_house_values\n",
    "quantile_pipeline = make_pipeline(num_imputer, quantile_transformer, standard_scaler)\n",
    "\n",
    "# Additional Features\n",
    "ratio_features_pipeline = make_pipeline(num_imputer, ratio_features_transformer,standard_scaler) \n",
    "\n",
    "# Piping!\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"bedrooms\",            ratio_features_pipeline,    [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\",     ratio_features_pipeline,    [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\" ,   ratio_features_pipeline,    [\"population\", \"households\"]),\n",
    "    (\"log\",                 log_pipeline,               [\"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]),\n",
    "    (\"geo\",                 geo_pipeline,               [\"latitude\", \"longitude\"]),\n",
    "    (\"cat\",                 cat_pipeline,               make_column_selector(dtype_include=object)),\n",
    "    (\"age\",                 quantile_pipeline,          [\"housing_median_age\"])\n",
    "    ], remainder=deafult_number_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = housing.drop([\"median_house_value\"], axis=1)\n",
    "labels = housing[\"median_house_value\"]\n",
    "\n",
    "\n",
    "# Creating a custom stratification based on the median house value\n",
    "labels_binned = pd.cut(labels, bins=20, labels=[x for x in range(1,21)])\n",
    "\n",
    "# Splitting the data into training and testing sets using stratified sampling\n",
    "X, X_test, y, y_test = train_test_split(features, labels, test_size=0.2, stratify=labels_binned, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\tRMSE: 68,955\tCV_RMSE: 71,648\n",
      "DecisionTreeRegressor\tRMSE: 0\tCV_RMSE: 66,074\n",
      "RandomForestRegressor\tRMSE: 17,285\tCV_RMSE: 47,140\n",
      "SVR\tRMSE: 118,139\tCV_RMSE: 118,192\n"
     ]
    }
   ],
   "source": [
    "class Model():\n",
    "    def __init__(self, preprocessor, predictor):\n",
    "        self.predictor = make_pipeline(preprocessor, predictor)\n",
    "        self.predictor_name = predictor.__class__.__name__\n",
    "\n",
    "    def fit_predict(self, dataset, labels):\n",
    "        self.predictor.fit(dataset, labels)\n",
    "        self.predictions = self.predictor.predict(dataset)\n",
    "        self.cv_rmses = -cross_val_score(self.predictor, dataset, labels, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "        self.cv_rmse = self.cv_rmses.mean()\n",
    "        self.rmse = root_mean_squared_error(labels, self.predictions)\n",
    "        return f\"{self.predictor_name}\\tRMSE: {self.rmse:,.0f}\\tCV_RMSE: {self.cv_rmse:,.0f}\"\n",
    "\n",
    "\n",
    "\n",
    "lin_reg = Model(preprocessing, LinearRegression())\n",
    "tree_reg = Model(preprocessing, DecisionTreeRegressor(random_state=42))\n",
    "forest_reg = Model(preprocessing, RandomForestRegressor(random_state=42))\n",
    "sv_reg = Model(preprocessing, SVR(C=1.0, epsilon=0.1, kernel=\"rbf\", degree=3, gamma=\"scale\"))\n",
    "\n",
    "results = []\n",
    "results.append(lin_reg.fit_predict(dataset=X, labels=y))\n",
    "results.append(tree_reg.fit_predict(dataset=X, labels=y))\n",
    "results.append(forest_reg.fit_predict(dataset=X, labels=y))\n",
    "results.append(sv_reg.fit_predict(dataset=X, labels=y))\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(search):\n",
    "    for param, value in search.best_params_.items():\n",
    "        print(f\"best {param.split('__')[-1]} found: {value}\")\n",
    "    print(f\"best RMSE: {-search.best_score_:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth found: 13\n",
      "best max_features found: 15\n",
      "best min_samples_leaf found: 10\n",
      "best min_samples_split found: 6\n",
      "best n_clusters found: 14\n",
      "best RMSE: 55,285\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline for Decision Tree Regressor\n",
    "tree_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),  # Step for preprocessing\n",
    "    (\"decision_tree\", DecisionTreeRegressor(random_state=42))  # Decision Tree Regressor with fixed random state for reproducibility\n",
    "])\n",
    "\n",
    "# Define the parameter distributions for randomized search\n",
    "tree_param_distribs = {\n",
    "    \"decision_tree__max_depth\": randint(low=3, high=15),       # Range of possible tree depths\n",
    "    \"decision_tree__min_samples_split\": randint(low=2, high=10), # Minimum number of samples to split an internal node\n",
    "    \"decision_tree__min_samples_leaf\": randint(low=3, high=15),  # Minimum number of samples allowed in a leaf node\n",
    "    \"decision_tree__max_features\": randint(low=5, high=20),      # Number of features to consider for best split\n",
    "    \"preprocessing__geo__cluster_simil_transformer__n_clusters\": randint(low=12, high=15)  # Number of clusters in the geo feature preprocessing step\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with 5-fold cross-validation\n",
    "tree_rnd_search = RandomizedSearchCV(\n",
    "    tree_pipeline, param_distributions=tree_param_distribs, n_iter=10,\n",
    "    cv=5, scoring='neg_root_mean_squared_error', random_state=42  # Using negative RMSE as scoring metric\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "tree_rnd_search.fit(X, y)\n",
    "show_result(tree_rnd_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 14\n",
      "best max_depth found: 10\n",
      "best max_features found: 7\n",
      "best min_samples_leaf found: 2\n",
      "best min_samples_split found: 5\n",
      "best RMSE: 47,761\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline for Random Forest Regressor\n",
    "forest_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),  # Step for preprocessing\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=42))  # Random Forest Regressor with fixed random state for reproducibility\n",
    "])\n",
    "\n",
    "# Define the parameter distributions for randomized search\n",
    "forest_param_distribs = {\n",
    "    \"random_forest__max_depth\": randint(low=2, high=15),         # Range of possible tree depths in the forest\n",
    "    \"random_forest__max_features\": randint(low=1, high=10),      # Number of features to consider at each split\n",
    "    \"random_forest__min_samples_split\": randint(low=2, high=6),  # Minimum number of samples to split an internal node\n",
    "    \"random_forest__min_samples_leaf\": randint(low=1, high=10),  # Minimum number of samples allowed in a leaf node\n",
    "    \"preprocessing__geo__cluster_simil_transformer__n_clusters\": randint(low=10, high=21),  # Number of clusters in geo feature preprocessing\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with 5-fold cross-validation\n",
    "forest_rnd_search = RandomizedSearchCV(\n",
    "    forest_pipeline, param_distributions=forest_param_distribs,\n",
    "    n_iter=10, cv=5, scoring=\"neg_root_mean_squared_error\", random_state=42  # Using negative RMSE as scoring metric\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "forest_rnd_search.fit(X, y)\n",
    "\n",
    "show_result(forest_rnd_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 7\n",
      "best C found: 10.0\n",
      "best degree found: 5\n",
      "best epsilon found: 0.0001\n",
      "best kernel found: linear\n",
      "best RMSE: 89,057\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline for Support Vector Regressor (SVR)\n",
    "svr_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),  # Step for preprocessing\n",
    "    (\"svr\", SVR())  # SVR model without fixed random state (no random_state parameter in SVR)\n",
    "])\n",
    "\n",
    "# Define the parameter distributions for randomized search\n",
    "svr_param_distribs = {\n",
    "    \"svr__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],         # Various kernel types to explore\n",
    "    \"svr__C\": np.logspace(-3, 3, 7),                             # Regularization parameter, using logspace for wide range of values\n",
    "    \"svr__epsilon\": np.logspace(-4, 0, 5),                       # Epsilon-tube parameter for the loss function\n",
    "    \"svr__degree\": randint(low=2, high=6),                       # Degree of the polynomial kernel function (if kernel='poly')\n",
    "    \"preprocessing__geo__cluster_simil_transformer__n_clusters\": randint(low=2, high=15)  # Number of clusters in geo feature preprocessing\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with 5-fold cross-validation\n",
    "svr_rnd_search = RandomizedSearchCV(\n",
    "    svr_pipeline, param_distributions=svr_param_distribs,\n",
    "    n_iter=10, cv=5, scoring=\"neg_root_mean_squared_error\", random_state=42  # Using negative RMSE as scoring metric\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "svr_rnd_search.fit(X, y)\n",
    "\n",
    "show_result(svr_rnd_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regressor Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Linear Regression, there are generally fewer hyperparameters to tune compared to models like Decision Trees, Random Forests, or SVR. Linear regression doesn't have parameters like max_depth or kernel, but we can still incorporate hyperparameter tuning by focusing on regularization techniques, like Ridge and Lasso regression, which are extensions of linear regression with L2 and L1 regularization, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SImple linear regression Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 14\n",
      "best RMSE: 71,144\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline for Linear Regression\n",
    "linear_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"linear_regression\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Define the parameter distribution (Linear Regression doesn't have many hyperparameters)\n",
    "linear_param_distribs = {\n",
    "    \"preprocessing__geo__cluster_simil_transformer__n_clusters\": randint(low=2, high=15),\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "linear_rnd_search = RandomizedSearchCV(\n",
    "    linear_pipeline,\n",
    "    param_distributions=linear_param_distribs,\n",
    "    n_iter=10, cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the randomized search to the data\n",
    "linear_rnd_search.fit(X, y)\n",
    "show_result(linear_rnd_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 14\n",
      "best alpha found: 193.06977288832496\n",
      "best RMSE: 69,477\n"
     ]
    }
   ],
   "source": [
    "ridge_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"standard_scaler\",StandardScaler()),\n",
    "    (\"ridge\", Ridge())\n",
    "])\n",
    "\n",
    "ridge_param_distribs = {\n",
    "    \"ridge__alpha\": np.logspace(2, 3, 15),  # Regularization strength (alpha)\n",
    "    \"preprocessing__geo__cluster_simil_transformer__n_clusters\": randint(low=2, high=15),\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "ridge_rnd_search = RandomizedSearchCV(\n",
    "    ridge_pipeline,\n",
    "    param_distributions=ridge_param_distribs,\n",
    "    n_iter=10, cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the randomized search to the data\n",
    "ridge_rnd_search.fit(X, y)\n",
    "show_result(ridge_rnd_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100.        ,  102.35310219,  104.76157528,  107.2267222 ,\n",
       "        109.74987655,  112.3324033 ,  114.97569954,  117.68119524,\n",
       "        120.45035403,  123.28467394,  126.18568831,  129.1549665 ,\n",
       "        132.19411485,  135.30477746,  138.48863714,  141.74741629,\n",
       "        145.08287785,  148.49682623,  151.9911083 ,  155.56761439,\n",
       "        159.22827933,  162.97508346,  166.81005372,  170.73526475,\n",
       "        174.75284   ,  178.86495291,  183.07382803,  187.38174229,\n",
       "        191.79102617,  196.304065  ,  200.92330026,  205.65123083,\n",
       "        210.49041445,  215.443469  ,  220.51307399,  225.70197196,\n",
       "        231.01297001,  236.44894126,  242.01282648,  247.7076356 ,\n",
       "        253.5364494 ,  259.50242114,  265.60877829,  271.85882427,\n",
       "        278.25594022,  284.80358684,  291.50530628,  298.36472403,\n",
       "        305.38555088,  312.57158497,  319.92671378,  327.45491629,\n",
       "        335.16026509,  343.04692863,  351.11917342,  359.38136638,\n",
       "        367.83797718,  376.49358068,  385.35285937,  394.42060594,\n",
       "        403.70172586,  413.20124001,  422.92428744,  432.87612811,\n",
       "        443.06214576,  453.48785081,  464.15888336,  475.08101621,\n",
       "        486.26015801,  497.70235643,  509.41380148,  521.4008288 ,\n",
       "        533.66992312,  546.22772177,  559.08101825,  572.23676594,\n",
       "        585.70208181,  599.48425032,  613.59072734,  628.02914418,\n",
       "        642.80731173,  657.93322466,  673.41506578,  689.26121043,\n",
       "        705.48023107,  722.08090184,  739.07220335,  756.46332755,\n",
       "        774.26368268,  792.48289835,  811.13083079,  830.21756813,\n",
       "        849.75343591,  869.74900262,  890.21508545,  911.16275612,\n",
       "        932.60334688,  954.54845666,  977.0099573 , 1000.        ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(2, 3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Regression Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 14\n",
      "best RMSE: 69,735\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline for Lasso Regression\n",
    "lasso_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", LassoCV(cv=5, random_state=42, max_iter=10000, tol=1e-3))\n",
    "])\n",
    "\n",
    "# Define the parameter distribution for randomized search\n",
    "lasso_param_distribs = {\n",
    "    \"preprocessing__geo__cluster_simil_transformer__n_clusters\": randint(low=2, high=15),\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "lasso_rnd_search = RandomizedSearchCV(\n",
    "    lasso_pipeline,\n",
    "    param_distributions=lasso_param_distribs,\n",
    "    n_iter=10, cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the randomized search to the data\n",
    "lasso_rnd_search.fit(X, y)\n",
    "show_result(lasso_rnd_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth found: 13\n",
      "best max_features found: 15\n",
      "best min_samples_leaf found: 10\n",
      "best min_samples_split found: 6\n",
      "best n_clusters found: 14\n",
      "best RMSE: 55,285\n"
     ]
    }
   ],
   "source": [
    "# These parameters are fine-tuned based on results from the previous random search\n",
    "tree_param_grid = [\n",
    "    {\n",
    "        \"decision_tree__max_depth\": [13], # Max depth of the tree: limits how deep the tree can grow\n",
    "        \"decision_tree__max_features\": [15], # Number of features to consider at each split\n",
    "        \"decision_tree__min_samples_leaf\": [10],  # Controls overfitting by ensuring minimum samples at leaf nodes\n",
    "        \"decision_tree__min_samples_split\": [6], # Ensures a node can only split if it has enough samples\n",
    "        \"preprocessing__geo__cluster_simil_transformer__n_clusters\": [14],\n",
    "    }\n",
    "]\n",
    "\n",
    "# This is a follow-up to the random search, aiming to find the optimal parameters in a more focused manner\n",
    "tree_grid_search = GridSearchCV(\n",
    "    tree_pipeline,  # Pipeline that includes preprocessing and model\n",
    "    tree_param_grid,  # Parameter grid to search through\n",
    "    refit=True,  # Refits the best model found on the entire dataset after grid search\n",
    "    scoring=\"neg_root_mean_squared_error\",  # Scoring metric to evaluate performance\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    error_score=\"raise\"  # Raise an error if fitting fails for a given parameter combination\n",
    ")\n",
    "\n",
    "tree_grid_search.fit(X, y)\n",
    "show_result(tree_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth found: 12\n",
      "best max_features found: 14\n",
      "best min_samples_leaf found: 9\n",
      "best min_samples_split found: 2\n",
      "best n_clusters found: 14\n",
      "best RMSE: 54,910\n"
     ]
    }
   ],
   "source": [
    "(tree_md, tree_mf, tree_msl, tree_mss, tree_nc) = (0, 0, 0, 0, 0)  # Initial parameter values\n",
    "\n",
    "tree_best_params = tree_grid_search.best_params_  # Retrieve best parameters from the initial grid search\n",
    "\n",
    "# Initialize flag to control the loop (set to False initially to ensure loop runs)\n",
    "flag = False  \n",
    "\n",
    "while not flag:\n",
    "    # Update parameters with the best ones found so far using max() to ensure valid values\n",
    "    tree_md = max(tree_best_params['decision_tree__max_depth'], 2)  # Ensure max_depth is at least 1 (we use [tree_med-1,...] in the param grid)\n",
    "    tree_mss = max(tree_best_params['decision_tree__min_samples_split'], 3)  # Ensure min_samples_split is at least 2\n",
    "    tree_msl = max(tree_best_params['decision_tree__min_samples_leaf'], 2)  # Ensure min_samples_leaf is at least 1\n",
    "    tree_mf = max(tree_best_params['decision_tree__max_features'], 2)  # Ensure max_features is at least 1\n",
    "    tree_nc = max(tree_best_params['preprocessing__geo__cluster_simil_transformer__n_clusters'], 3)  # Ensure n_clusters is at least 2\n",
    "    \n",
    "    # Define the new, smaller grid around the current best parameters\n",
    "    tree_param_grid = [\n",
    "        {\n",
    "            \"decision_tree__max_depth\": [tree_md-1, tree_md, tree_md+1],     \n",
    "            \"decision_tree__min_samples_split\": [tree_mss-1, tree_mss, tree_mss+1],     \n",
    "            \"decision_tree__min_samples_leaf\": [tree_msl-1, tree_msl, tree_msl+1],     \n",
    "            \"decision_tree__max_features\": [tree_mf-1, tree_mf, tree_mf+1],\n",
    "            \"preprocessing__geo__cluster_simil_transformer__n_clusters\": [tree_nc-1, tree_nc, tree_nc+1],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Perform a new grid search with the refined parameter grid\n",
    "    tree_grid_search = GridSearchCV(\n",
    "        tree_pipeline, tree_param_grid, refit=True, scoring=\"neg_root_mean_squared_error\", cv=5, error_score=\"raise\"\n",
    "    )\n",
    "    \n",
    "    tree_grid_search.fit(X, y)\n",
    "    \n",
    "    # Get the new best parameters after this iteration\n",
    "    new_best_params = tree_grid_search.best_params_\n",
    "    \n",
    "    # Check if parameters have stopped changing (i.e., they are equal to the previous best)\n",
    "    flag = tree_best_params == new_best_params\n",
    "    \n",
    "    # Update the best parameters for the next iteration if flag is still False\n",
    "    tree_best_params = new_best_params\n",
    "    show_result(tree_grid_search)\n",
    "# After the loop exits, we will have the most fine-tuned parameters in tree_best_params\n",
    "show_result(tree_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 20\n",
      "best max_depth found: 18\n",
      "best max_features found: 6\n",
      "best min_samples_leaf found: 2\n",
      "best min_samples_split found: 2\n",
      "best RMSE: 42,613\n"
     ]
    }
   ],
   "source": [
    "# These parameters are fine-tuned based on results from the previous Random Search for Random Forest\n",
    "forest_param_grid = [\n",
    "    {\n",
    "        \"random_forest__max_depth\": [17, 18, 19], # 10, 11, 12, 13, 14, 15, 16, 17, 18\n",
    "        \"random_forest__max_features\": [5, 6, 7], # 7, 6, 5, 6\n",
    "        \"random_forest__min_samples_leaf\": [2, 3], # 5, 4 , 3, 2, 2\n",
    "        \"random_forest__min_samples_split\": [2, 3], # 5, 4, 3, 2, 2\n",
    "        \"preprocessing__geo__cluster_simil_transformer__n_clusters\": [19, 20, 21], #14, 15, 16, 17, 18, 19, 20\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize GridSearchCV to perform grid search over the specified parameters\n",
    "forest_grid_search = GridSearchCV(\n",
    "    forest_pipeline,\n",
    "    forest_param_grid,\n",
    "    refit=True,\n",
    "    scoring=\"neg_root_mean_squared_error\",  # cross-validation is being applied to evaluate the model's performance\n",
    "                                            # using the negative root mean squared error (RMSE) as the scoring metric\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    error_score=\"raise\"  # Raise an error if fitting fails for a given parameter combination\n",
    ")\n",
    "\n",
    "forest_grid_search.fit(X, y)\n",
    "show_result(forest_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code would take too much time to run, so I only wrote it to provide another step in the fune-tuning process. Therefore I manipulated the previous code several times manually, starting from two values per parameter at each step, and finally running a grid search with three parameters per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 9\n",
      "best max_depth found: 12\n",
      "best max_features found: 6\n",
      "best min_samples_leaf found: 1\n",
      "best min_samples_split found: 3\n",
      "best RMSE: 64,096\n",
      "best n_clusters found: 9\n",
      "best max_depth found: 12\n",
      "best max_features found: 7\n",
      "best min_samples_leaf found: 2\n",
      "best min_samples_split found: 2\n",
      "best RMSE: 63,000\n",
      "best n_clusters found: 9\n",
      "best max_depth found: 12\n",
      "best max_features found: 8\n",
      "best min_samples_leaf found: 1\n",
      "best min_samples_split found: 3\n",
      "best RMSE: 62,763\n"
     ]
    }
   ],
   "source": [
    "(forest_md, forest_mf, forest_msl, forest_mss, forest_nc) = (0, 0, 0, 0, 0) # Initial parameter values\n",
    "\n",
    "# Retrieve the best parameters from the previous grid search\n",
    "forest_best_params = forest_grid_search.best_params_\n",
    "\n",
    "# Initialize flag to control the loop (set to False initially to ensure loop runs)\n",
    "flag = False \n",
    "\n",
    "# Loop until no better parameters are found\n",
    "while not flag:\n",
    "    # Update parameter values based on the best parameters found\n",
    "    forest_md = max(forest_best_params['random_forest__max_depth'], 2)\n",
    "    forest_mss = max(forest_best_params['random_forest__min_samples_split'], 3)\n",
    "    forest_msl = max(forest_best_params['random_forest__min_samples_leaf'], 2)\n",
    "    forest_mf = max(forest_best_params['random_forest__max_features'], 2)\n",
    "    forest_nc = max(forest_best_params['preprocessing__geo__cluster_simil_transformer__n_clusters'], 3)\n",
    "    \n",
    "    # Define the parameter grid for further optimization\n",
    "    forest_param_grid = [\n",
    "        {\n",
    "            # Explore values for max_depth around the best found value\n",
    "            \"random_forest__max_depth\": [forest_md - 1, forest_md, forest_md + 1],     \n",
    "            # Explore values for min_samples_split around the best found value\n",
    "            \"random_forest__min_samples_split\": [forest_mss - 1, forest_mss, forest_mss + 1],     \n",
    "            # Explore values for min_samples_leaf around the best found value\n",
    "            \"random_forest__min_samples_leaf\": [forest_msl - 1, forest_msl, forest_msl + 1],     \n",
    "            # Explore values for max_features around the best found value\n",
    "            \"random_forest__max_features\": [forest_mf - 1, forest_mf, forest_mf + 1],\n",
    "            # Explore values for n_clusters for the geo clustering transformer\n",
    "            \"preprocessing__geo__cluster_simil_transformer__n_clusters\": [forest_nc - 1, forest_nc, forest_nc + 1],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Initialize GridSearchCV to perform a grid search over the updated parameters\n",
    "    forest_grid_search = GridSearchCV(\n",
    "        forest_pipeline,  # The pipeline that includes preprocessing and the model\n",
    "        forest_param_grid,  # The parameter grid to search through\n",
    "        refit=True,  # Refits the best model found on the entire dataset after grid search\n",
    "        scoring=\"neg_root_mean_squared_error\",  # Scoring metric for evaluating performance\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        error_score=\"raise\"  # Raise an error if fitting fails for a given parameter combination\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search on the provided dataset (X, y)\n",
    "    forest_grid_search.fit(X, y)\n",
    "\n",
    "   # Get the new best parameters after this iteration\n",
    "    new_best_params = forest_grid_search.best_params_\n",
    "    \n",
    "    # Check if parameters have stopped changing (i.e., they are equal to the previous best)\n",
    "    flag = forest_best_params == new_best_params\n",
    "    \n",
    "    # Update the best parameters for the next iteration if flag is still False\n",
    "    forest_best_params = new_best_params\n",
    "    show_result(forest_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although the algorithm could not complete its execution, the first three iterations of the loop were successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 8\n",
      "best C found: 20.0\n",
      "best degree found: 4\n",
      "best epsilon found: 1e-05\n",
      "best kernel found: linear\n",
      "best RMSE: 83,267\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters from RandomizedSearchCV\n",
    "best_n_clusters = 7\n",
    "best_C = 10.0\n",
    "best_degree = 5\n",
    "best_epsilon = 0.0001\n",
    "best_kernel = 'linear'\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "svr_param_grid = {\n",
    "    \"svr__kernel\": [best_kernel],  # Fixed to the best kernel ('linear' in this case)\n",
    "    \"svr__C\": [best_C / 2, best_C, best_C * 2],  # Refine around the best C found\n",
    "    \"svr__epsilon\": [best_epsilon / 10, best_epsilon, best_epsilon * 10],  # Fine-tune epsilon\n",
    "    \"svr__degree\": [best_degree - 1, best_degree, best_degree + 1],  # Only matters for 'poly', kept for generality\n",
    "    \"preprocessing__geo__cluster_simil_transformer__n_clusters\": [best_n_clusters - 1, best_n_clusters, best_n_clusters + 1]  # Narrow search for clusters\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 5-fold cross-validation\n",
    "svr_grid_search = GridSearchCV(\n",
    "    svr_pipeline, param_grid=svr_param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\", cv=5, error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "svr_grid_search.fit(X, y)\n",
    "\n",
    "# Display the results\n",
    "show_result(svr_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (Ridge) Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n_clusters found: 18\n",
      "best alpha found: 188.06977288832496\n",
      "best RMSE: 68,944\n"
     ]
    }
   ],
   "source": [
    "# Use initial values for the parameters from random search\n",
    "ridge_alpha, ridge_nc = (\n",
    "    ridge_rnd_search.best_params_['ridge__alpha'],\n",
    "    ridge_rnd_search.best_params_['preprocessing__geo__cluster_simil_transformer__n_clusters'])  # Initial values for alpha and n_clusters\n",
    "\n",
    "# Retrieve best parameters from RandomizedSearchCV\n",
    "ridge_best_params = ridge_rnd_search.best_params_\n",
    "\n",
    "# Define a flag to check if parameters have changed, initializing it to False to ensure that the loop will run\n",
    "flag = False\n",
    "\n",
    "while not flag:\n",
    "    \n",
    "    ridge_alpha = ridge_best_params['ridge__alpha']\n",
    "    ridge_nc = ridge_best_params['preprocessing__geo__cluster_simil_transformer__n_clusters']\n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    ridge_param_grid = {\n",
    "        \"ridge__alpha\": [ridge_alpha-5, ridge_alpha, ridge_alpha+5],  # Refine around the best alpha found\n",
    "        \"preprocessing__geo__cluster_simil_transformer__n_clusters\": [ridge_nc-1,ridge_nc, ridge_nc+1]  # Narrow search for clusters\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV with 5-fold cross-validation\n",
    "    ridge_grid_search = GridSearchCV(\n",
    "        ridge_pipeline, param_grid=ridge_param_grid,\n",
    "        scoring=\"neg_root_mean_squared_error\", cv=5, error_score=\"raise\"\n",
    "    )\n",
    "\n",
    "    # Fit the model to the data\n",
    "    ridge_grid_search.fit(X, y)\n",
    "\n",
    "    # Update best parameters\n",
    "    ridge_best_params = ridge_grid_search.best_params_\n",
    "\n",
    "    # Check if the best parameters have changed\n",
    "    flag = (ridge_alpha == ridge_best_params['ridge__alpha'] and \n",
    "            ridge_nc == ridge_best_params['preprocessing__geo__cluster_simil_transformer__n_clusters'])\n",
    "\n",
    "    # Update the parameters\n",
    "    \n",
    "show_result(ridge_grid_search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
